<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Research</title>
    <link rel="icon" href="assets/favicon.svg" type="image/svg+xml" />
    <link rel="stylesheet" href="assets/styles.css" />
    <script defer src="assets/script.js"></script>
  </head>
  <body class="page" data-theme="dark">
    <div class="page-wrap">
      <div class="page-top">
        <a href="index.html">HOME</a>
        <button class="page-toggle" type="button" data-theme-toggle aria-pressed="false">
          INVERSE COLORS
        </button>
      </div>

      <h1>RESEARCH</h1>

      <p class="small">
        I study continuous functions in physics and computational chemistry,
        with a current focus on continuous representations to model physical
        quantities such as DFT electron densities.
      </p>

      <p>
        I’m also interested in representation learning: for materials and
        molecules, how can we represent atoms and structure more richly than
        categorical types, to enable zero- or few-shot generative modeling for
        material discovery?
      </p>

      <p>
        Questions or collaborations? Email me via the <a href="about.html">About</a> page.
      </p>

      <h2>Selected Publications</h2>
      <ul class="pubs">
        <li>
          <a href="https://www.arxiv.org/abs/2601.21583" target="_blank" rel="noopener">
            CORDS: Continuous Representations of Discrete Structures
          </a>
          <div class="pubs__meta">
            Hadži Veljković et al., 2026 — Invertible mapping from variable-size sets to continuous density + feature fields, enabling models to operate in field space while decoding exactly back to discrete objects; evaluated on molecular generation, detection, and simulation-based inference.
          </div>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2504.03521" target="_blank" rel="noopener">
            Dynamic Training Enhances Machine Learning Potentials for Long-Lasting Molecular Dynamics
          </a>
          <div class="pubs__meta">
            Žugec, Hadži Veljković et al., 2025 — Introduces dynamic training to refresh ML interatomic potentials during long MD runs, boosting EGNN accuracy on hydrogen–palladium/graphene systems and maintaining fidelity over extended simulations.
          </div>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2405.20915" target="_blank" rel="noopener">
            Fast yet Safe: Early-Exiting with Risk Control
          </a>
          <div class="pubs__meta">
            Jazbec, Timans, Hadži Veljković et al., 2024 — Applies distribution-free risk control to early-exit neural nets so exits trigger only when predictions meet quality guarantees, delivering speedups without sacrificing target accuracy across vision and language tasks.
          </div>
        </li>
      </ul>
    </div>
  </body>
</html>
